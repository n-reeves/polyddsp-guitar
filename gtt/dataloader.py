import io 
import os
import sys

import numpy as np

import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn import functional as F
import torchaudio
import pretty_midi

from gtt.utilities.utils import load_audio, crop_audio

from basic_pitch.inference import predict, Model
from basic_pitch import ICASSP_2022_MODEL_PATH

from PolyDDSP.modules.loudness import LoudnessExtractor


class Recording:
    """
    Simple class storing audio (and when applicable midi) file names
    
    Args:
        audio_name: name of the audio file
        midi_name: name of the midi file containing labels
    Out:
        torch tensor containing audio file at sample rate
    """
    def __init__(self,
                 audio_name: str ,
                 midi_name: str = ""):
        self.audio_name = audio_name
        self.midi_name = midi_name
        
    def set_midi_name(self, midi_name):
        self.midi_name = midi_name
    

#note feature extractor is currently built aroudn the use of a dataloader. Likely needs refactor for general usage outside of experiments
class FeatureExtractor:
    """
    Class for extracting features from audio and midi files
    
    Inputs:
        sr: sample rate of the audio
        synth_unit_frames: number of frames generated by DDSP modules for each control
        pitch_extractor_type: type of pitch extractor used. 'bp' for basic pitch, any other string for midi
        trim_duration: seconds at the start at end that are excluded (used to remove fully silent sections of audio)
        max_n_pitch: maximum number of unique midi notes alloed in pitch and amplitude features in an audio segment
        segment_length_seconds: length of each audio segment in seconds to be processed by feature extractor
        loud_mean: mean of the loudness values used for normalization
        loud_std: standard deviation of the loudness values used for normalization
        audio_dir: path to the folder containing audio files
        full_amps: boolean to include full amplitude data in the output (used for visualization in one figure)
        permute_pitch: boolean to shuffle F0 features row wise. acts as additional form of data augmentation for global control layers
        device: device to move features to
    
    """
    def __init__(self,
                 sr: int = 22050,
                 synth_unit_frames: int = 64,
                 pitch_extractor_type: str = 'bp',
                 trim_duration: int = 5,
                 max_n_pitch: int = 30,
                 segment_length_seconds: float = 4,
                 loud_mean: float = 0,
                 loud_std: float = 1,
                 audio_dir: str = '',
                 full_amps: bool = False,
                 permute_pitch: bool = True,
                 device: str = 'cpu'
                ):
        
        self.sr = sr
        self.synth_unit_frames = synth_unit_frames
        self.pitch_extractor_type = pitch_extractor_type
        self.audio_dir = audio_dir
        self.device = device
        self.segment_length_seconds = segment_length_seconds
        self.num_measures = int(np.ceil(sr*segment_length_seconds/synth_unit_frames))
        self.loud_mean = loud_mean
        self.loud_std = loud_std
        self.full_amps = full_amps
        self.permute_pitch = permute_pitch
        
        #Calculate number of loudness and pitch measures needed per second
        self.measures_second = np.ceil(self.sr/synth_unit_frames)
        self.trim_duration = trim_duration
        self.trim_duration_samples = self.trim_duration*self.sr
        self.max_n_pitch = max_n_pitch
        
        self.loudness_extractor = LoudnessExtractor(sr=self.sr,
                                                    device = 'cpu')
        
        if self.pitch_extractor_type == 'bp':
            self.basic_pitch = self.basic_pitch = Model(ICASSP_2022_MODEL_PATH, )
        
        
    def get_features(self, audio, audio_start_index=0, midi_path = '' ):
        """
        returns dictionary containing audio, pitch, amplitude and loudness features
        
        Args:
            audio: torch tensor containing audio data of shape (1,samples) 
            audio_start_index: starting index of the audio segment in the full audio file
            midi_path: path to the midi file containing pitch labels
        Returns: 
            dictionary containing 
                audio: shape (batch,1,samples), 
                pitch: shape (batch,max_n_pitch,features in audio segment ), 
                amplitude: shape (batch,max_n_pitch,features in audio segment )
                loudness: shape (batch,1,features in audio segment)
        
        """      
        loudness = self.loudness_extractor(audio)
        loudness = (loudness - self.loud_mean)/self.loud_std
        #add upsample loudness
        loudness = F.interpolate(loudness.unsqueeze(0), size=self.num_measures, mode='linear').squeeze(0)
        
        if self.pitch_extractor_type == "bp":
            placeholder_path = os.path.join(self.audio_dir ,'placeholder_crop.wav')
            torchaudio.save(placeholder_path, audio ,self.sr)
            
            old_out = sys.stdout
            supress_print = io.StringIO()
            sys.stdout = supress_print
            
            _, midi_data, _ = predict(placeholder_path, self.basic_pitch)
            sys.stdout = old_out
            
            os.remove(placeholder_path)
            
            # Remove pitch bends from midi_data
            for instrument in midi_data.instruments:
                instrument.pitch_bends = []
            
            #add silent note to create uniform midi length if audio ends in silence
            if midi_data.get_end_time() < self.segment_length_seconds:
                
                #add silent note if midi file cuts off early
                silent_note = pretty_midi.Note(velocity=0,
                                               pitch=0, 
                                               start=midi_data.get_end_time(), 
                                               end=self.segment_length_seconds)
                
                #add midi instrument to handle edge cases where file has no instrument present
                if len(midi_data.instruments) == 0:
                    instrument_program = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')
                    instrument = pretty_midi.Instrument(program=instrument_program)
                    midi_data.instruments.append(instrument)
                
                midi_data.instruments[0].notes.append(silent_note)
            
            piano_roll = midi_data.get_piano_roll(fs=self.measures_second)
            
        #use midi file to produce pitch data
        else:
            midi_data = pretty_midi.PrettyMIDI(midi_path)
            
            segment_length_seconds = audio.shape[-1]/self.sr
            
            if midi_data.get_end_time() < self.segment_length_seconds:
                silent_note = pretty_midi.Note(velocity=0,
                                               pitch=0, 
                                               start=midi_data.get_end_time(), 
                                               end=segment_length_seconds)
                
                if len(midi_data.instruments) == 0:
                    instrument_program = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')
                    instrument = pretty_midi.Instrument(program=instrument_program)
                    midi_data.instruments.append(instrument)
                
                midi_data.instruments[0].notes.append(silent_note)
            
            piano_roll = midi_data.get_piano_roll(fs=self.measures_second)
            
            start_ind_scaled = int((audio_start_index+self.trim_duration_samples)/self.sr*self.measures_second)
            end_ind_scaled = int(start_ind_scaled + self.measures_second*segment_length_seconds)
            piano_roll = piano_roll[:,start_ind_scaled:end_ind_scaled]
        
        #undo amplitude to midi velocity used in bp
        #may need adjustment depending on framework used for pitch encoder
        amplitudes = np.copy(piano_roll[21:109,:])/127
        frequencies = np.copy(piano_roll)
        
        for i, row in enumerate(frequencies):
            freq = 440 * 2**((i-69)/12) 
    
            non_zero = np.nonzero(frequencies[i,:])[0]
            frequencies[i,non_zero] = freq
        
        frequencies = frequencies[21:109,:]
        
        full_amplitudes = amplitudes
        #mask and filter non zero rows for innactive pitch classes
        non_zero_mask = np.copy(frequencies)
        frequencies = frequencies[~np.all(non_zero_mask == 0, axis=1)]
        amplitudes = amplitudes[~np.all(non_zero_mask == 0, axis=1)]
        
        #add padding
        if frequencies.shape[0] < self.max_n_pitch:
            to_pad = self.max_n_pitch - frequencies.shape[0]
            padding = np.zeros((to_pad,frequencies.shape[1]))
            
            frequencies = np.vstack((frequencies,padding))
            amplitudes = np.vstack((amplitudes,padding))
            
        
        frequencies = torch.from_numpy(frequencies[:self.max_n_pitch,:])
        amplitudes = torch.from_numpy(amplitudes[:self.max_n_pitch,:])
        
        #crop pitch and velocity data based on number of measures
        amplitudes = amplitudes[:,:self.num_measures]
        frequencies = frequencies[:,:self.num_measures] 
        
        #shuffle pitch and velocity channels
        if self.permute_pitch:
            permutation = np.random.permutation(frequencies.shape[0])

            amplitudes = amplitudes[permutation]
            frequencies = frequencies[permutation]
            
        
        features = {'audio':audio.squeeze(0).to(self.device),
              'amplitude':amplitudes.float().to(self.device),
              'pitch':frequencies.float().to(self.device),
              'loudness':loudness.float().to(self.device)  }
        
        #for visualization
        if self.full_amps:
            features['full_amps'] = full_amplitudes
        
        return features

    
class GttDataset(Dataset):
    """
    Class for loading audio/midi data

    Args:
        audio_dir: path to the folder of the .wav files
        midi_dir: path to .mid or .midi files. basic pitch transciption is used to produce transcription when left blank
        file_list: list of audio file names to load
        sr: sample rate of the audio (resamples to this value)
        segment_length_seconds: length of the segments in the dataset
        random_crop: randomly select starting position in full clip
        trim_duration: seconds at the start at end that are excluded (used to remove fully silent sections of audio
        synth_unit_frames: number of frames generated by DDSP modules for each control
        max_n_pitch: maximum number of unique midi notes allowed in pitch and amplitude features in an audio segment
        loud_mean: mean of the loudness values used for normalization
        loud_std: standard deviation of the loudness values used for normalization
        full_amps: boolean to include full amplitude data in the output (used for visualization in one figure)
        permute_pitch: boolean to shuffle F0 features row wise. acts as additional form of data augmentation for global control layers
        device: device to move features to
    
    Returns:
        Dictionary containing features produced by FeatureExtractor class
            
    """
    def __init__(self,
                 audio_dir: str,
                 midi_dir: str = "",
                 file_list: list = [],
                 sr: int = 22050,
                 segment_length_seconds: int = 4,
                 random_crop : bool = True,
                 trim_duration: int = 5,
                 synth_unit_frames: int = 64,
                 max_n_pitch: int = 30,
                 loud_mean: float = 0,
                 loud_std: float = 1,
                 full_amps: bool = False,
                 permute_pitch: bool = True,
                 device: str = 'cpu'
                ):
        self.audio_dir = audio_dir
        self.midi_dir = midi_dir
        self.device = device
        
        self.sr = sr
        self.synth_unit_frames = synth_unit_frames
        self.segment_length_seconds = segment_length_seconds
        self.segment_samples = self.sr * self.segment_length_seconds
        self.random_crop = random_crop
        self.trim_duration_samples = trim_duration*self.sr
        
        self.pitch_extractor_type = 'bp' if midi_dir == '' else ''
        
        self.feature_extractor = FeatureExtractor(sr = self.sr,
                                                  synth_unit_frames = self.synth_unit_frames,
                                                  pitch_extractor_type = self.pitch_extractor_type,
                                                  trim_duration = trim_duration,
                                                  audio_dir = self.audio_dir,
                                                  max_n_pitch = max_n_pitch,
                                                  segment_length_seconds = segment_length_seconds,
                                                  loud_mean = loud_mean,
                                                  loud_std = loud_std,
                                                  full_amps = full_amps,
                                                  permute_pitch = permute_pitch,
                                                  device = self.device)
        
        #load file audio filenames
        if len(file_list) == 0:
            audio_files = os.listdir(self.audio_dir)
        else:
            audio_files = file_list
        
        audio_file_names = [os.path.splitext(x)[0] for x in audio_files if x.endswith('.wav')]
    
        self.recordings = [Recording(x) for x in audio_files if x.endswith('.wav')]
        
        #load midi file names
        if midi_dir != "":
            midi_dir_content = os.listdir(self.midi_dir)
            midi_files = []
            
            for midi in midi_dir_content:
                for recording in self.recordings:
                    if ((midi.endswith('.mid') or midi.endswith('.midi')) 
                        and os.path.splitext(midi)[0] == os.path.splitext(recording.audio_name)[0]):
                        
                        recording.set_midi_name(midi)
                        midi_files.append(midi)
            
            if len(midi_files) != len(audio_file_names):
                for filename in midi_files:
                    if os.path.splittext(filename)[0] not in audio_file_names:
                        print('Missing midi file for {}'.format(filename))
                raise Exception('Missing midi files')
        
    def __len__(self):
        return len(self.recordings)  
    
    def __getitem__(self, idx):
        
        recording = self.recordings[idx]
        
        audio_path = os.path.join(self.audio_dir,recording.audio_name)
        
        audio = load_audio(filepath = audio_path, sr = self.sr)
        
        start_ind = self.trim_duration_samples
        end_ind = audio.shape[-1] - self.trim_duration_samples
        
        trimmed_audio = audio[...,start_ind:end_ind]
        
        cropped_audio_out = crop_audio(audio = trimmed_audio,
                                  segment_length = self.segment_samples,
                                  random_crop = self.random_crop)
        
        cropped_audio = cropped_audio_out['audio']
        
        if self.midi_dir != "":
            midi_path = os.path.join(self.midi_dir, recording.midi_name)
            out = self.feature_extractor.get_features(cropped_audio, 
                                                     audio_start_index = cropped_audio_out['start_ind'],
                                                     midi_path = midi_path)
            
        else:
            out = self.feature_extractor.get_features(cropped_audio, 
                                                     audio_start_index = cropped_audio_out['start_ind'])
        
        return out